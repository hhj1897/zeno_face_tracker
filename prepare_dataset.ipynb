{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare_dataset\n",
    "\n",
    "Track Zeno's face images using an [exisiting tool](https://github.com/IntelligentBehaviourUnderstandingGroup/dlib_and_chehra_stuff) and prepare them for manual annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import dlib\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "try:\n",
    "    from ConfigParser import ConfigParser    # If using Python 2.7\n",
    "except ImportError:\n",
    "    from configparser import ConfigParser    # If using Python 3.5\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "sys.path.append(os.path.realpath(config.get('facial_landmark_tracker', 'repository_path')))\n",
    "import ibug_face_tracker\n",
    "from zeno_face_tracker_helpers import *\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialise the landmark localiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facial landmark localiser initialised.\n"
     ]
    }
   ],
   "source": [
    "tracker = ibug_face_tracker.FaceTracker(os.path.realpath(config.get('facial_landmark_tracker', \n",
    "                                                                    'ert_model_path')), \n",
    "                                        os.path.realpath(config.get('facial_landmark_tracker', \n",
    "                                                                    'auxiliary_model_path')))\n",
    "tracker.face_detection_scale = config.getfloat('facial_landmark_tracker', 'face_detection_scale')\n",
    "tracker.minimum_face_size = config.getint('facial_landmark_tracker', 'minimum_face_size')\n",
    "tracker.hard_failure_threshold = -1e6\n",
    "tracker.estimate_head_pose = False\n",
    "tracker.eye_iterations = 0\n",
    "print('Facial landmark localiser initialised.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Localise Facial Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 images to be processed.\n",
      "78 images have been processed.\n",
      "135 images have been processed.\n",
      "191 images have been processed.\n",
      "266 images have been processed.\n",
      "330 images have been processed.\n",
      "384 images have been processed.\n",
      "450 images have been processed.\n",
      "520 images have been processed.\n",
      "579 images have been processed.\n",
      "634 images have been processed.\n",
      "693 images have been processed.\n",
      "755 images have been processed.\n",
      "809 images have been processed.\n",
      "861 images have been processed.\n",
      "926 images have been processed.\n",
      "981 images have been processed.\n",
      "1036 images have been processed.\n",
      "1099 images have been processed.\n",
      "1159 images have been processed.\n",
      "Done, all 1200 images have been processed, 1034 of which have landmarks localised.\n"
     ]
    }
   ],
   "source": [
    "# Enumerate jobs\n",
    "source_images = sorted(glob.glob(os.path.realpath(os.path.join('./dataset', 'cam*', '*.png'))))\n",
    "source_images = [x for x in source_images if '.pts.png' not in x]\n",
    "print('%d images to be processed.' % len(source_images))\n",
    "\n",
    "# Get landmarks\n",
    "successes = 0\n",
    "last_check_time = time.time()\n",
    "for idx, image_path in enumerate(source_images):\n",
    "    image = cv2.imread(image_path)\n",
    "    tracker.reset()\n",
    "    tracker.track(image)\n",
    "    if tracker.has_facial_landmarks:\n",
    "        pts_path = os.path.splitext(image_path)[0] + '.init.pts'\n",
    "        save_pts(pts_path, tracker.facial_landmarks)\n",
    "        tracker.plot_current_result(image)\n",
    "        rendering_path = pts_path + '.png'\n",
    "        cv2.imwrite(rendering_path, image)\n",
    "        successes += 1\n",
    "    current_time = time.time()\n",
    "    if current_time - last_check_time > 10.0:\n",
    "        last_check_time = current_time\n",
    "        print('%d images have been processed.' % (idx + 1))\n",
    "print('Done, all %d images have been processed, %d of which have landmarks localised.' % \n",
    "      (len(source_images), successes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare the annotation jobs than can be loaded in FLAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_000.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_000.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_001.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_001.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_002.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_002.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_003.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_003.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_004.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_004.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_005.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_005.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_006.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_006.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_007.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_007.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_008.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_008.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_009.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_009.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_010.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_010.fad.bak\n",
      "Annotation job created: C:\\zeno_face_tracker\\dataset\\batch_011.fad\n",
      "Backup file created: C:\\zeno_face_tracker\\dataset\\batch_011.fad.bak\n"
     ]
    }
   ],
   "source": [
    "# Enumerate the recording sessions\n",
    "recording_sessions = sorted(glob.glob(os.path.realpath(os.path.join('./dataset', 'cam*'))))\n",
    "recording_sessions = [x for x in recording_sessions if os.path.isdir(x)]\n",
    "\n",
    "# How many examples per session?\n",
    "num_samples = 1e6\n",
    "for session in recording_sessions:\n",
    "    images = glob.glob(os.path.join(session, '*.png'))\n",
    "    images = [x for x in images if '.pts.' not in x]\n",
    "    num_samples = min(num_samples, len(images))\n",
    "\n",
    "# Permutate samples in the sessions so that each annotation batch gets a bit of everything\n",
    "indices = np.zeros((len(recording_sessions), num_samples), dtype=int)\n",
    "for idx in range(indices.shape[1]):\n",
    "    indices[:, idx] = np.random.permutation(indices.shape[0])\n",
    "\n",
    "# Create jobs\n",
    "for batch_idx, batch in enumerate(indices):\n",
    "    batch_content = []\n",
    "    for idx, session in enumerate(batch):\n",
    "        image_path = os.path.join(recording_sessions[session], '%06d.png' % idx)\n",
    "        pts_path = os.path.splitext(image_path)[0] + '.init.pts'\n",
    "        if os.path.exists(pts_path):\n",
    "            pts = load_pts(pts_path)\n",
    "            if pts.shape[0] == 68:\n",
    "                batch_content.append((image_path, pts))\n",
    "    job_path = os.path.realpath(os.path.join('./dataset', 'batch_%03d.fad' % batch_idx))\n",
    "    duplication = 0\n",
    "    while True:\n",
    "        if os.path.exists(job_path):\n",
    "            duplication += 1\n",
    "            job_path = os.path.realpath(os.path.join(\n",
    "                './dataset', 'batch_%03d.%03d.fad' % (batch_idx, duplication)))\n",
    "        else:\n",
    "            break\n",
    "    save_annotation_job(job_path, batch_content, 68)\n",
    "    print('Annotation job created: ' + job_path)\n",
    "    backup_path = job_path + '.bak'\n",
    "    shutil.copyfile(job_path, backup_path)\n",
    "    print('Backup file created: ' + backup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5: Do something about the undetected faces: prepare for face box annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 leftover images have been processed.\n",
      "All 166 leftover images have been processed.\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_012.face_box.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_012.face_box.fad.bak\n"
     ]
    }
   ],
   "source": [
    "# See what had been included in previous batches\n",
    "number_of_sessions = len(glob.glob(os.path.realpath(os.path.join('./dataset', 'cam*.pkl'))))\n",
    "batch_paths = [os.path.realpath(os.path.join('./dataset', 'batch_%03d.fad' % x)) \n",
    "               for x in range(number_of_sessions)]\n",
    "included_images = []\n",
    "for batch_path in batch_paths:\n",
    "    batch_data = load_annotation_job(batch_path, config.getint('data_organisation', \n",
    "                                                               'number_of_landmarks'))\n",
    "    included_images += [x['image_path'] for x in batch_data]\n",
    "\n",
    "# Get the leftover images\n",
    "leftover_images = sorted(glob.glob(os.path.realpath(os.path.join('./dataset', 'cam*', '*.png'))))\n",
    "leftover_images = [x for x in leftover_images if '.pts.png' not in x and x not in included_images]\n",
    "\n",
    "# Do something about these images\n",
    "face_detection_scale = config.getfloat('facial_landmark_tracker', 'face_detection_scale')\n",
    "face_detection_scale = max(face_detection_scale, 1e-6)\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "last_check_time = time.time()\n",
    "batch_content = []\n",
    "for idx, leftover_image in enumerate(leftover_images):\n",
    "    index = int(os.path.basename(leftover_image).split('.')[0])\n",
    "    face_boxes = None\n",
    "    for delta in [-1, 1]:\n",
    "        next_index = index\n",
    "        while True:\n",
    "            next_index += delta\n",
    "            next_image_path = os.path.join(os.path.dirname(leftover_image), '%06d.png' % next_index)\n",
    "            if os.path.exists(next_image_path):\n",
    "                if next_image_path in included_images:\n",
    "                    next_image = cv2.cvtColor(cv2.imread(next_image_path), cv2.COLOR_BGR2GRAY)\n",
    "                    image_size = (next_image.shape[1], next_image.shape[0])\n",
    "                    target_size = (int(max(round(image_size[0] * face_detection_scale), 1)), \n",
    "                                   int(max(round(image_size[1] * face_detection_scale), 1)))\n",
    "                    if target_size != image_size:\n",
    "                        next_image = cv2.resize(next_image, target_size)\n",
    "                    detected_faces = sorted(\n",
    "                        [dlib.rectangle(int(round(face_box.left() / face_detection_scale)), \n",
    "                                        int(round(face_box.top() / face_detection_scale)), \n",
    "                                        int(round(face_box.right() / face_detection_scale)), \n",
    "                                        int(round(face_box.bottom() / face_detection_scale))) \n",
    "                         for face_box in face_detector(next_image)], \n",
    "                        key=dlib.rectangle.area, reverse=True)\n",
    "                    if len(detected_faces) > 0:\n",
    "                        next_face_box = np.array([detected_faces[0].left(), \n",
    "                                                  detected_faces[0].top(), \n",
    "                                                  detected_faces[0].right(), \n",
    "                                                  detected_faces[0].bottom()])\n",
    "                        if face_boxes is None:\n",
    "                            face_boxes = np.expand_dims(next_face_box, 0)\n",
    "                        else:\n",
    "                            face_boxes = np.vstack((face_boxes, next_face_box))\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "    estimated_face_box = face_boxes.mean(axis=0)\n",
    "    batch_content.append((leftover_image, \n",
    "                          np.array([[estimated_face_box[0], estimated_face_box[1]], \n",
    "                                    [estimated_face_box[2], estimated_face_box[1]], \n",
    "                                    [estimated_face_box[2], estimated_face_box[3]], \n",
    "                                    [estimated_face_box[0], estimated_face_box[3]]])))\n",
    "    current_time = time.time()\n",
    "    if last_check_time < current_time - 10.0:\n",
    "        last_check_time = current_time\n",
    "        print('%d leftover images have been processed.' % (idx + 1))\n",
    "print('All %d leftover images have been processed.' % len(leftover_images))\n",
    "\n",
    "# Prepare the FAD file\n",
    "job_path = os.path.realpath(os.path.join(\n",
    "    './dataset', 'batch_%03d.face_box.fad' % number_of_sessions))\n",
    "duplication = 0\n",
    "while True:\n",
    "    if os.path.exists(job_path):\n",
    "        duplication += 1\n",
    "        job_path = os.path.realpath(os.path.join(\n",
    "            './dataset', 'batch_%03d.%03d.face_box.fad' % (number_of_sessions, duplication)))\n",
    "    else:\n",
    "        break\n",
    "save_annotation_job(job_path, batch_content, 4)\n",
    "print('Annotation job created: ' + job_path)\n",
    "backup_path = job_path + '.bak'\n",
    "shutil.copyfile(job_path, backup_path)\n",
    "print('Backup file created: ' + backup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Do something about the undetected faces: prepare for landmark annotation\n",
    "\n",
    "__Do not run this before completing the face box annotation work!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 samples have been loaded.\n",
      "150 leftover images have been processed.\n",
      "Done, all 166 leftover images have been processed, 166 of which have landmarks localised.\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_012.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_012.fad.bak\n"
     ]
    }
   ],
   "source": [
    "# Get face box from the FAD file\n",
    "samples = []\n",
    "fad_paths = sorted(glob.glob(os.path.realpath(\n",
    "    os.path.join('./dataset', 'batch*.face_box.fad'))))\n",
    "for fad_path in fad_paths:\n",
    "    samples += load_annotation_job(fad_path, 4)\n",
    "print('%d samples have been loaded.' % len(samples))\n",
    "\n",
    "# Get landmarks\n",
    "successes = 0\n",
    "batch_content = []\n",
    "last_check_time = time.time()\n",
    "for idx, sample in enumerate(samples):\n",
    "    image = cv2.imread(sample['image_path'])\n",
    "    tracker.reset()\n",
    "    top_left = np.round(sample['facial_landmarks'].min(axis=0)).astype(int)\n",
    "    bottom_right = np.round(sample['facial_landmarks'].max(axis=0)).astype(int)\n",
    "    tracker.track(image, (top_left[0], top_left[1], bottom_right[0] - top_left[0] + 1, \n",
    "                          bottom_right[1] - top_left[1] + 1))\n",
    "    if tracker.has_facial_landmarks:\n",
    "        batch_content.append((sample['image_path'], tracker.facial_landmarks))\n",
    "        pts_path = os.path.splitext(sample['image_path'])[0] + '.init.pts'\n",
    "        save_pts(pts_path, tracker.facial_landmarks)\n",
    "        tracker.plot_current_result(image)\n",
    "        rendering_path = pts_path + '.png'\n",
    "        cv2.imwrite(rendering_path, image)\n",
    "        successes += 1\n",
    "    current_time = time.time()\n",
    "    if current_time - last_check_time > 10.0:\n",
    "        last_check_time = current_time\n",
    "        print('%d leftover images have been processed.' % (idx + 1))\n",
    "print('Done, all %d leftover images have been processed, %d of which have landmarks localised.' % \n",
    "      (len(samples), successes))\n",
    "\n",
    "# Prepare the FAD file\n",
    "number_of_sessions = len(glob.glob(os.path.realpath(os.path.join('./dataset', 'cam*.pkl'))))\n",
    "job_path = os.path.realpath(os.path.join(\n",
    "    './dataset', 'batch_%03d.fad' % number_of_sessions))\n",
    "duplication = 0\n",
    "while True:\n",
    "    if os.path.exists(job_path):\n",
    "        duplication += 1\n",
    "        job_path = os.path.realpath(os.path.join(\n",
    "            './dataset', 'batch_%03d.%03d.fad' % (number_of_sessions, duplication)))\n",
    "    else:\n",
    "        break\n",
    "save_annotation_job(job_path, batch_content, 68)\n",
    "print('Annotation job created: ' + job_path)\n",
    "backup_path = job_path + '.bak'\n",
    "shutil.copyfile(job_path, backup_path)\n",
    "print('Backup file created: ' + backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
