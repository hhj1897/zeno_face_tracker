{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_models\n",
    "\n",
    "Train the face detection and landmark localisation models for Zeno's face based on our annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import dlib\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    from ConfigParser import ConfigParser    # If using Python 2.7\n",
    "except ImportError:\n",
    "    from configparser import ConfigParser    # If using Python 3.5\n",
    "config = ConfigParser()\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the data structure for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 face detection samples and 202 landmark localisation have been prepared.\n",
      "924 face detection samples and 403 landmark localisation have been prepared.\n",
      "All 1200 face detection samples and 515 landmark localisation have been prepared.\n"
     ]
    }
   ],
   "source": [
    "# Load images and annotation\n",
    "config.read('config.ini')\n",
    "annotations = pd.read_pickle(os.path.realpath(os.path.join('./dataset', 'annotations.pkl')))\n",
    "face_detection_scale = config.getfloat('facial_landmark_tracker', 'face_detection_scale')\n",
    "face_detection_images = []\n",
    "face_detection_groundtruth = []\n",
    "shape_predictor_images = []\n",
    "shape_predictor_groundtruth = []\n",
    "last_check_time = time.time()\n",
    "for idx in range(annotations.shape[0]):\n",
    "    entry = annotations.iloc[idx]\n",
    "    image_path = os.path.realpath(os.path.join('./dataset', entry['session'], \n",
    "                                               '%06d.png' % entry['index']))\n",
    "    if type(entry['face_box']) != type(np.nan) or not np.isnan(entry['face_box']):\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY)\n",
    "        image_size = (image.shape[1], image.shape[0])\n",
    "        face_detection_size = (max(round(image_size[0] * face_detection_scale), 1), \n",
    "                               max(round(image_size[1] * face_detection_scale), 1))\n",
    "        if face_detection_size != image_size:\n",
    "            face_detection_images.append(cv2.resize(image, face_detection_size))\n",
    "        else:\n",
    "            face_detection_images.append(face_detection_image)\n",
    "        face_box = [int(round(x * face_detection_scale)) for x in list(entry['face_box'])]\n",
    "        face_detection_groundtruth.append([dlib.rectangle(face_box[0], face_box[1], \n",
    "                                                          face_box[0] + face_box[2] - 1, \n",
    "                                                          face_box[1] + face_box[3] - 1)])\n",
    "        if type(entry['landmarks']) != type(np.nan) or not np.isnan(entry['landmarks']):\n",
    "            shape_predictor_images.append(image)\n",
    "            bounding_box = dlib.rectangle(entry['face_box'][0], entry['face_box'][1], \n",
    "                                          entry['face_box'][0] + entry['face_box'][2] - 1, \n",
    "                                          entry['face_box'][1] + entry['face_box'][3] - 1)\n",
    "            landmarks = [dlib.point(int(round(pts[0])), int(round(pts[1]))) \n",
    "                         for pts in entry['landmarks']]\n",
    "            shape_predictor_groundtruth.append([dlib.full_object_detection(bounding_box, \n",
    "                                                                           landmarks)])\n",
    "    current_time = time.time()\n",
    "    if last_check_time < current_time - 10.0:\n",
    "        last_check_time = current_time\n",
    "        print('%d face detection samples and %d landmark localisation have been prepared.' % \n",
    "              (len(face_detection_groundtruth), len(shape_predictor_groundtruth)))\n",
    "print('All %d face detection samples and %d landmark localisation have been prepared.' % \n",
    "      (len(face_detection_groundtruth), len(shape_predictor_groundtruth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split the data into a training set (2/3) and a validation set (1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / test split for face detection: 800 / 400.\n",
      "Train / test split for landmark localisation: 343 / 172.\n"
     ]
    }
   ],
   "source": [
    "# For face detection\n",
    "sample_indices = np.random.permutation(len(face_detection_images))\n",
    "training_set_size = int(round(len(face_detection_images) / 3 * 2))\n",
    "face_detection_training_images = [face_detection_images[idx] for idx in \n",
    "                                  sample_indices[0:training_set_size]]\n",
    "face_detection_training_groundtruth = [face_detection_groundtruth[idx] for idx in \n",
    "                                       sample_indices[0:training_set_size]]\n",
    "face_detection_validation_images = [face_detection_images[idx] for idx in \n",
    "                                    sample_indices[training_set_size:]]\n",
    "face_detection_validation_groundtruth = [face_detection_groundtruth[idx] for idx in \n",
    "                                         sample_indices[training_set_size:]]\n",
    "print('Train / test split for face detection: %d / %d.' % \n",
    "      (len(face_detection_training_images), len(face_detection_validation_images)))\n",
    "\n",
    "# For landmark localisation\n",
    "sample_indices = np.random.permutation(len(shape_predictor_images))\n",
    "training_set_size = int(round(len(shape_predictor_images) / 3 * 2))\n",
    "shape_predictor_training_images = [shape_predictor_images[idx] for idx in \n",
    "                                   sample_indices[0:training_set_size]]\n",
    "shape_predictor_training_groundtruth = [shape_predictor_groundtruth[idx] for idx in \n",
    "                                        sample_indices[0:training_set_size]]\n",
    "shape_predictor_validation_images = [shape_predictor_images[idx] for idx in \n",
    "                                     sample_indices[training_set_size:]]\n",
    "shape_predictor_validation_groundtruth = [shape_predictor_groundtruth[idx] for idx in \n",
    "                                          sample_indices[training_set_size:]]\n",
    "print('Train / test split for landmark localisation: %d / %d.' % \n",
    "      (len(shape_predictor_training_images), len(shape_predictor_validation_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train and test the face detector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detector model has been trained and saved to: D:\\hhj\\zeno_face_tracker\\models\\zeno_face_detector.model\n",
      "Dlib face detector model has been saved to: D:\\hhj\\zeno_face_tracker\\models\\dlib_face_detector.model\n"
     ]
    }
   ],
   "source": [
    "config.read('config.ini')\n",
    "options = dlib.simple_object_detector_training_options()\n",
    "options.be_verbose = config.getboolean('simple_object_detector_training_options', 'be_verbose')\n",
    "options.add_left_right_image_flips = config.getboolean('simple_object_detector_training_options', \n",
    "                                                       'add_left_right_image_flips')\n",
    "options.num_threads = config.getint('simple_object_detector_training_options', 'num_threads')\n",
    "options.detection_window_size = config.getint('simple_object_detector_training_options', \n",
    "                                              'detection_window_size')\n",
    "options.C = config.getfloat('simple_object_detector_training_options', 'C')\n",
    "options.epsilon = config.getfloat('simple_object_detector_training_options', 'epsilon')\n",
    "options.upsample_limit = config.getint('simple_object_detector_training_options', \n",
    "                                       'upsample_limit')\n",
    "options.nuclear_norm_regularization_strength = config.getfloat('simple_object_detector_training_options', \n",
    "                                                               'nuclear_norm_regularization_strength')\n",
    "# face_detector = dlib.train_simple_object_detector(face_detection_training_images, \n",
    "#                                                   face_detection_training_groundtruth, \n",
    "#                                                   options)\n",
    "# print(dlib.test_simple_object_detector(face_detection_validation_images, \n",
    "#                                        face_detection_validation_groundtruth, face_detector))\n",
    "face_detector = dlib.train_simple_object_detector(face_detection_images, \n",
    "                                                  face_detection_groundtruth, \n",
    "                                                  options)\n",
    "face_detector_model_path = os.path.realpath(os.path.join('./models', 'zeno_face_detector.model'))\n",
    "face_detector.save(face_detector_model_path)\n",
    "print('Face detector model has been trained and saved to: ' + face_detector_model_path)\n",
    "dlib_face_deector_model_path = os.path.realpath(os.path.join('./models', 'dlib_face_detector.model'))\n",
    "dlib.get_frontal_face_detector().save(dlib_face_deector_model_path)\n",
    "print('Dlib face detector model has been saved to: ' + dlib_face_deector_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the shape predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dlib.shape_predictor_training_options()\n",
    "options.be_verbose = config.getboolean('shape_predictor_training_options', \n",
    "                                       'be_verbose')\n",
    "options.cascade_depth = config.getint('shape_predictor_training_options', \n",
    "                                      'cascade_depth')\n",
    "options.feature_pool_region_padding = config.getfloat('shape_predictor_training_options', \n",
    "                                                      'feature_pool_region_padding')\n",
    "options.feature_pool_size = config.getint('shape_predictor_training_options', \n",
    "                                          'feature_pool_size')\n",
    "options.lambda_param = config.getfloat('shape_predictor_training_options', \n",
    "                                       'lambda_param')\n",
    "options.nu = config.getfloat('shape_predictor_training_options', 'nu')\n",
    "options.num_test_splits = config.getint('shape_predictor_training_options', \n",
    "                                        'num_test_splits')\n",
    "options.num_trees_per_cascade_level = config.getint('shape_predictor_training_options', \n",
    "                                                    'num_trees_per_cascade_level')\n",
    "options.oversampling_amount = config.getint('shape_predictor_training_options', \n",
    "                                            'oversampling_amount')\n",
    "options.tree_depth = config.getint('shape_predictor_training_options', 'tree_depth')\n",
    "shape_predictor = dlib.train_shape_predictor(shape_predictor_images, \n",
    "                                             shape_predictor_groundtruth, \n",
    "                                             options)\n",
    "shape_predictor_model_path = os.path.realpath(os.path.join('./models', 'zeno_face_tracker.model'))\n",
    "shape_predictor.save(shape_predictor_model_path)\n",
    "print('Shape predictor model has been trained and saved to: ' + shape_predictor_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the detector model on live video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam #0 opened.\n"
     ]
    }
   ],
   "source": [
    "face_detector_model_path = os.path.realpath(os.path.join(\n",
    "    './models', 'zeno_face_detector.model'))\n",
    "dlib_face_deector_model_path = os.path.realpath(os.path.join(\n",
    "    './models', 'dlib_face_detector.model'))\n",
    "face_detector = dlib.simple_object_detector(face_detector_model_path)\n",
    "webcam = cv2.VideoCapture(0)\n",
    "if webcam.isOpened():\n",
    "    print('Webcam #0 opened.')\n",
    "    while True:\n",
    "        _, frame = webcam.read()\n",
    "        if frame.ndim == 3 and frame.shape[2] == 3:\n",
    "            face_detection_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            face_detection_frame = frame\n",
    "        face_boxes = face_detector(face_detection_frame)\n",
    "        for face_box in face_boxes:\n",
    "            cv2.rectangle(frame, (face_box.left(), face_box.top()), \n",
    "                         (face_box.right(), face_box.bottom()), \n",
    "                         color=(0, 0, 255), thickness=2, lineType=cv2.LINE_AA)\n",
    "        cv2.imshow('Webcam #0', frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q') or key == ord('Q'):\n",
    "            break\n",
    "    webcam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print('Failed to open webcam #0.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dlib.simple_object_detector_training_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simple_object_detector_training_options(be_verbose=0, add_left_right_image_flips=0, num_threads=4, detection_window_size=6400, C=1, epsilon=0.01, max_runtime_seconds=3.1536e+09, upsample_limit=2, nuclear_norm_regularization_strength=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dlib.shape_predictor_training_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape_predictor_training_options(be_verbose=0, cascade_depth=10, tree_depth=4, num_trees_per_cascade_level=500, nu=0.1, oversampling_amount=20, oversampling_translation_jitter=0, feature_pool_size=400, lambda_param=0.1, num_test_splits=20, feature_pool_region_padding=0, random_seed=, num_threads=0, landmark_relative_padding_mode=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simple_object_detector_training_options(be_verbose=1, add_left_right_image_flips=1, num_threads=6, detection_window_size=6400, C=5, epsilon=0.01, max_runtime_seconds=3.1536e+09, upsample_limit=2, nuclear_norm_regularization_strength=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(rectangles[[(209, 209) (424, 424)]], [2.1619599499238844], [1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlib.fhog_object_detector.run_multiple([face_detector, default_face_detector], face_detection_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 1, recall: 0.9725, average precision: 0.9725\n"
     ]
    }
   ],
   "source": [
    "print(dlib.test_simple_object_detector(face_detection_validation_images, \n",
    "                                 face_detection_validation_groundtruth, face_detector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[rectangle(261,128,410,277)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detection_groundtruth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rectangles[[(305, 281) (520, 496)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to cast Python instance to C++ type (compile in debug mode for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-86066ccd23b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m face_detector_model_path = os.path.realpath(os.path.join(\n\u001b[0;32m      2\u001b[0m     './models', 'zeno_face_detector.model'))\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mface_detector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_object_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to cast Python instance to C++ type (compile in debug mode for details)"
     ]
    }
   ],
   "source": [
    "face_detector_model_path = os.path.realpath(os.path.join(\n",
    "    './models', 'zeno_face_detector.model'))\n",
    "face_detector = dlib.simple_object_detector([dlib.get_frontal_face_detector()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dlib.simple_object_detector at 0x9131228>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to cast Python instance to C++ type (compile in debug mode for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b8780588922b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     './models', 'dlib_face_detector.model'))\n\u001b[0;32m      5\u001b[0m face_detector = dlib.simple_object_detector([dlib.simple_object_detector(face_detector_model_path),\n\u001b[1;32m----> 6\u001b[1;33m                                              dlib.fhog_object_detector(dlib_face_deector_model_path)])\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to cast Python instance to C++ type (compile in debug mode for details)"
     ]
    }
   ],
   "source": [
    "face_detector_model_path = os.path.realpath(os.path.join(\n",
    "    './models', 'zeno_face_detector.model'))\n",
    "dlib_face_deector_model_path = os.path.realpath(os.path.join(\n",
    "    './models', 'dlib_face_detector.model'))\n",
    "face_detector = dlib.simple_object_detector([dlib.simple_object_detector(face_detector_model_path),\n",
    "                                             dlib.fhog_object_detector(dlib_face_deector_model_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19.15.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\hhj\\\\zeno_face_tracker\\\\models\\\\dlib_face_detector.model'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlib_face_deector_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 1, recall: 0.845, average precision: 0.845\n"
     ]
    }
   ],
   "source": [
    "print(dlib.test_simple_object_detector(face_detection_validation_images, \n",
    "                                       face_detection_validation_groundtruth, dlib.get_frontal_face_detector()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to cast Python instance to C++ type (compile in debug mode for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6b1e4e259c73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdlib_face_deector_model_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./models'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dlib_face_detector.svm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_object_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdlib_face_deector_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dlib face detector model has been saved to: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdlib_face_deector_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to cast Python instance to C++ type (compile in debug mode for details)"
     ]
    }
   ],
   "source": [
    "dlib_face_deector_model_path = os.path.realpath(os.path.join('./models', 'dlib_face_detector.svm'))\n",
    "dlib.simple_object_detector([dlib.get_frontal_face_detector()]).save(dlib_face_deector_model_path)\n",
    "print('Dlib face detector model has been saved to: ' + dlib_face_deector_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lala = (dlib.fhog_object_detector(dlib_face_deector_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lala.num_detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detector.num_detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xe8 in position 17: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-04e4963f76a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdlib_face_deector_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xe8 in position 17: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "aa = pickle.load(open(dlib_face_deector_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
